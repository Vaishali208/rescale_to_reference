Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                              count
-----------------------------  -------
adjust_reference_distribution        1
all                                  1
calculate_max_fragment_length        1
plot_graph                           1
total                                4

Select jobs to execute...

[Sun Nov  3 11:06:21 2024]
rule calculate_max_fragment_length:
    input: shuf.a.bed.gz
    output: data/max_fragment_length.txt
    jobid: 3
    reason: Missing output files: data/max_fragment_length.txt
    resources: tmpdir=/tmp

[Sun Nov  3 11:06:43 2024]
Error in rule calculate_max_fragment_length:
    jobid: 3
    input: shuf.a.bed.gz
    output: data/max_fragment_length.txt
    shell:
        
        zcat shuf.a.bed.gz > temp_unzipped.bed
        awk '{print $3 - $2}' temp_unzipped.bed > temp_lengths.txt
        sort -nr temp_lengths.txt | head -n 1 > data/max_fragment_length.txt
        rm temp_unzipped.bed temp_lengths.txt
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job calculate_max_fragment_length since they might be corrupted:
data/max_fragment_length.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-11-03T110620.750869.snakemake.log
